# Finetune mistral-7b with custom data
Mistral 7B is a new 7.3 billion parameter language model that represents a major advance in large language model (LLM) capabilities. It has outperformed the 13 billion parameter Llama 2 model on all tasks and outperforms the 34 billion parameter Llama 1 on many benchmarks

HuggingFace's Transformers library has become the go-to platform for working with state-of-the-art NLP models like Mistral 7B. For this tutorial, we will utilize the Transformers library to handle tasks like tokenization and model initialization. 
To deal with computation constraints, we will leverage Peft—a library designed to accelerate deep learning models. We'll leverage QLoRA, which allows for awesome new 4-bit quanitization and paged optimizers, reducing VRAM requirements.

When it comes to training LLM’s, computational efficiency and performance are often at odds. You either need enormous computational resources to train large models from scratch or compromise the model's performance by settling for smaller architectures. However, I've been using a popular technique that offers the best of both worlds: Q-Lora.

Purpose of this is project is to finetune mistral 7b large language model and see how it performs on custom dataset. I use `databricks/databricks-dolly-15k` which is an open source dataset of instruction-following records generated by thousands of Databricks employees in several of the behavioral categories outlined in the InstructGPT paper, including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.

#### Install the necessary Library

First we need to install our necessary python library for this project-

        !pip install -q -U transformers 
        !pip install -q -U bitsandbytes 
        !pip install -q -U peft 
        !pip install -q -U datasets 
        !pip install -q -U accelerate trl

#### Load the model and the tokenizer

Next we need to load out model and tokenizer.When it comes to training LLM’s, computational efficiency and performance are often at odds. You either need enormous computational resources to train large models from scratch or compromise the model's performance by settling for smaller architectures. However, I've been using a popular technique that offers the best of both worlds: Q-Lora.

###### Lora
Before getting into QLoRA (quantized lora), it’s helpful first to understand Lora. The idea is that you can leave the original pre-trained backbone model intact, and add on additional layers which can be more efficiently trained. This allows for faster adaptability to new tasks without retraining the entire network. By focusing the learning on a subset of trainable parameters, you can retain the advantages of a large, pre-trained model while reducing the computational overhead significantly. This is especially advantageous in real-world applications where computational resources are limited or rapid adaptation to new data is required. Furthermore, the implementation of Lora opens up the possibility of having a single, universal model that can be fine-tuned for various tasks in a more storage- and compute-efficient manner.

![Lora](https://i.ytimg.com/vi/dA-NhCtrrVE/maxresdefault.jpg)

###### QLora

Alright, let me break down QLORA. This method is amazing for anyone who wants to fine-tune massive language models but doesn't have a supercomputer handy. 
The secret sauce? A few key innovations. First off, QLoRA uses a new data type called 4-bit NormalFloat. This data type is tailored for normally distributed weights and outperforms other 4-bit types. Then there's Double Quantization. This is essentially quantization for quantization; it quantizes the quantization constants, effectively reducing memory requirements even more. And don’t forget Paged Optimizers, which are there to make sure that memory spikes don’t throw a wrench in the works. Paged Optimizers utilize NVIDIA's unified memory feature to automatically transfer data between CPU and GPU, helping to prevent out-of-memory errors on the GPU.


![QLora](https://miro.medium.com/v2/resize:fit:1400/1*dZTXNIrb50tvzFT_nmV7uw.png)


###### BitsandBytes
The BitsandBytes library is integrated with Hugging Face’s Transformers library to simplify the process of model quantization.

Quantization is a technique used to reduce the precision of numerical values in a model. Instead of using high-precision data types like 32-bit floating-point numbers, quantization represents values using lower-precision data types, such as 8-bit integers. This process significantly reduces memory usage and can speed up model execution while maintaining acceptable accuracy.

The integration of Hugging Face’s Transformers library with the BitsandBytes library makes this technique more accessible and user-friendly.

BitsAndBytesConfig
The BitsAndBytesConfig configures the quantization process for a model, specifying that it should use 4-bit quantization with the NF4 data type and compute using the torch.bfloat16 data type. The nested quantization technique is also enabled for enhanced memory efficiency.

load_in_4bit=True: This argument indicates that the model should be loaded in 4-bit quantization. By doing so, memory usage can be reduced by approximately fourfold.
bnb_4bit_use_double_quant=True: This argument suggests using the nested quantization technique, which offers even greater memory efficiency without compromising performance.

bnb_4bit_quant_type="nf4": The NF4 data type is designed for weights initialized using a normal distribution. By specifying this type, the model uses the NF4 data type for quantization.

bnb_4bit_compute_dtype=torch.bfloat16: This argument allows you to modify the data type used during computation. By setting it to torch.bfloat16, it can result in speed improvements in specific scenarios.


        import torch
        from transformers import BitsAndBytesConfig
        from transformers import AutoTokenizer
        from transformers import AutoModelForCausalLM
        
        base_model = "mistralai/Mistral-7B-v0.1"
        
        tokenizer = AutoTokenizer.from_pretrained(
            base_model, 
            padding_side = "right",
            add_eos_token = True,
        )
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.add_bos_token, tokenizer.add_eos_token
        
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=False,
            bnb_4bit_compute_dtype=torch.bfloat16
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            base_model,
            load_in_4bit=True,
            quantization_config=bnb_config,
            torch_dtype=torch.bfloat16,
            device_map="auto",
        )

###### PEFT
Fine-tuning a model for each specific task by adapting all of its parameters can be impractical due to the large number of parameters in modern models.

In such scenarios, PEFT proposes techniques that only train a smaller subset of parameters or use low-rank adaptation (LoRA) methods to minimize the number of trainable parameters. This way, the process becomes more efficient and practical.

PEFT is a library created by HuggingFace that focuses on efficiently adjusting pre-trained language models for various downstream applications without fine-tuning all of the model's parameters.

The primary goal of PEFT is to address the increasing computational and storage costs associated with fine-tuning large-scale PLMs by fine-tuning only a small number of additional model parameters, significantly reducing these costs. Despite reducing the number of fine-tuned parameters, recent PEFT techniques have achieved performance comparable to full fine-tuning. PEFT provides tools and methods to fine-tune large language models more efficient and accessible, especially on consumer hardware.

It balances computational efficiency and model performance, making it a valuable tool for researchers and practitioners working with large-scale PLMs.

LoRA's approach to fine-tuning uses low-rank decomposition to represent weight updates with two smaller matrices.

This reduces the number of trainable parameters, making fine-tuning more efficient. The original pre-trained weights remain frozen, allowing multiple lightweight and portable LoRA models to be built on top of them for different tasks.

LoRA is compatible with many other parameter-efficient methods and you can stack methods.

Fine-tuned models using LoRA perform comparably to fully fine-tuned models, and LoRA doesn't add any inference latency. It can be applied to any subset of weight matrices in a neural network. Transformer models typically apply LoRA to attention blocks only.

The number of trainable parameters in a LoRA model depends on the size of the low-rank update matrices, determined mainly by the rank r and the shape of the original weight matrix.


        from peft import prepare_model_for_kbit_training
        from peft import LoraConfig, get_peft_model
        from peft import get_peft_model
        
        model.gradient_checkpointing_enable()
        
        model = prepare_model_for_kbit_training(model)
            
        lora_config = LoraConfig(
            r=8,
            lora_alpha=16,
            target_modules=[
                "q_proj",
                "k_proj",
                "v_proj",
                "o_proj",
                "gate_proj",
                "up_proj",
                "down_proj",
                "lm_head",
            ],
            bias="none",
            lora_dropout=0.05, 
            task_type="CAUSAL_LM",
        )
        
        model = get_peft_model(model, lora_config)

###### LoraConfig
This configuration is used to store the settings for a LoRA model, which is designed for fine-tuning a model with LoRA (Low Rank Adapters). The provided parameters are essential for determining how the LoRA layers behave during the fine-tuning process.

lora_alpha (int): This parameter represents the scaling factor for the weight matrices in LoRA, which is adjusted by alpha to control the magnitude of the combined output from the base model and low-rank adaptation.
r (int): This represents the LoRA rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters.
bias (str): This parameter specifies the bias type for LoRA. It can take values such as 'none', 'all', or 'lora_only'. If set to 'all' or 'lora_only', the corresponding biases will be updated during training. This means that even when disabling the adapters, the model might not produce the same output as the base model would have without adaptation. In the given code, the bias is set to "none", meaning no bias is used.
ℹ️ Note: The weight matrix is multiplied by lora_alpha/r, and a higher lora_alpha value assigns more weight to the LoRA activations.

For better performance, the HuggingFace docs recommend setting bias to None first, and then lora_only, before trying all.

lora_dropout (float): This parameter indicates the dropout probability for LoRA layers. Dropout is a regularization technique where randomly selected neurons are ignored during training, helping to prevent overfitting. In this configuration, the dropout rate is set to 0.1 or 10%.
task_type (str): This parameter indicates the type of task for which the model is being fine-tuned. In the provided code, it's set to CAUSAL_LM, which stands for causal language modeling.
prepare_model_for_kbit_training
This method outlines the protocol for preparing a model prior to training. The steps include casting layernorm in fp32, requiring grads for output embedding layer, and upcasting the LLMs head to fp32.


#### Prepare the dataset

For prepraing the dataset first i need to load the dataset and then process the dataset for the model.


        from datasets import load_dataset
        
        dataset_name = "databricks/databricks-dolly-15k"
        
        train_dataset = load_dataset(dataset_name, split="train[0:800]")
        eval_dataset = load_dataset(dataset_name, split="train[800:1000]")


For training the model i am going to to use instruction tuning. Instruction tuning is a process that improves the capabilities and controllability of LLMs by fine-tuning a base model using pairs of instructions and their corresponding outputs.This approach helps to align LLMs more closely with human instructions, making them more controllable, predictable, and adaptable without requiring extensive retraining.The instruction tuning process follows a clear pipeline, which involves training a base model on instruction-output pairs.This results in a more fine-tuned model that can better understand and respond to human instructions. This process allows LLMs to better interpret human language and produce more accurate and reliable outputs.

        def generate_prompt(sample):
            full_prompt =f"""<s>[INST]{sample['instruction']}
            {f"Here is some context: {sample['context']}" if len(sample["context"]) > 0 else None}
            [/INST] {sample['response']}</s>"""
            return {"text": full_prompt}
        
        generated_train_dataset = train_dataset.map(generate_prompt, remove_columns=list(train_dataset.features))
        generated_val_dataset = eval_dataset.map(generate_prompt, remove_columns=list(train_dataset.features))

#### Now train the model 

###### Initialize Training Arguments
num_train_epochs(float, defaults to 3.0): Total number of training epochs to perform (if not an integer, will perform the decimal part percents of the last epoch before stopping training).
per_device_train_batch_size: The batch size per device for training

gradient_accumulation_steps: Number of updates steps to accumulate the gradients for, before performing a backward/update pass.

gradient_checkpointing: If True, use gradient checkpointing to save memory at the expense of slower backward pass.

optim the optimizer to use. You can choose from:

        - `adamw_hf`
        - `adamw_torch`
        - `adamw_torch_fused`
        - `adamw_torch_xla`
        - `adamw_apex_fused`
        - `adafactor`
        - `adamw_anyprecision`
        - `sgd`
        - `adagrad`
        - `adamw_bnb_8bit`
        - `adamw_8bit` (just an alias for `adamw_bnb_8bit`)
        - `lion_8bit`
        - `lion_32bit`
        - `paged_adamw_32bit`
        - `paged_adamw_8bit`
        - `paged_lion_32bit`
        - `paged_lion_8bit`
        - `rmsprop`


save_steps: Number of updates steps before two checkpoint saves if save_strategy="steps". Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.

bf16: Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training.

max_grad_norm (float, *optional*, defaults to 1.0): Maximum gradient norm (for gradient clipping)

warmup_ratio (float, optional, default 0.0:Ratio of total training steps used for a linear warmup from 0 to learning_rate.

-lr_scheduler_type (defaults to linear): The scheduler type to use. You can choose from the following:

- `linear`
- `cosine`
- `cosine_with_restarts`
- `polynomial`
- `constant`
- `constant_with_warmup`
- `inverse_sqrt`
- `reduce_lr_on_plateau`



        from transformers import TrainingArguments
        
        training_arguments = TrainingArguments(
            output_dir="./results",
            num_train_epochs=1,
            per_device_train_batch_size=4,
            gradient_accumulation_steps=1,
            optim="paged_adamw_32bit",
            save_strategy="steps",
            save_steps=25,
            logging_steps=25,
            learning_rate=2e-4,
            weight_decay=0.001,
            max_steps=50,
            evaluation_strategy="steps", 
            eval_steps=25,       
            do_eval=True,               
            report_to="none",
        )
 Now we need to define the trainer. For this we hoing to use SFTTrainer from TRL library which is also a part of huggungface library. 

###### What is the TRL library?
The TRL (Transformers Reinforcement Learning) library is a tool that makes the Reinforcement Learning (RL) step in fine-tuning Large Language Models (LLMs) more straightforward and more flexible. It lets users fine-tune their language models using RL with their custom datasets and training setups. The library supports the Deep RL algorithm called PPO, which can be run distributed or on a single device.

TRL also uses the accelerate feature from the HuggingFace ecosystem, which allows users to scale up their experiments and achieve better results.

###### What problems does it solve?
The TRL library solves the challenges associated with fine-tuning Language Models (LLMs) using Reinforcement Learning (RL), mainly when dealing with memory constraints.

Training an LLM with Reinforcement Learning with Human Feedback (RLHF) involves several steps, such as fine-tuning a pre-trained LLM on a specific domain, collecting a human-annotated dataset to train a reward model, and further fine-tuning the LLM using the reward model and dataset with RL. TRL simplifies the RL step in this process, making it more accessible and efficient.

TRL also offers solutions to fit the setup on a single GPU, even with an increased model size. Integrating TRL with Parameter-Efficient Fine-Tuning (PEFT) allows for fine-tuning large LLMs using RLHF at a reasonable cost.

PEFT supports creating and fine-tuning adapter layers on LLMs, enabling fine-tuning with significantly reduced GPU memory requirements. By integrating TRL and PEFT, users can fine-tune massive models on a 24GB consumer GPU, which typically requires 40GB in bfloat16.

###### What is the SFTTrainer?
The SFTTrainer (Supervised Fine-tuning Trainer) is a class provided by the TRL (Transformers Reinforcement Learning) library.

It facilitates supervised fine-tuning, a crucial step in RLHF (Reinforcement Learning with Human Feedback). The SFTTrainer provides an easy-to-use API to create and train SFT models with just a few lines of code on a given dataset.

When initializing the SFTTrainer class, you pass the following:

    base model to be trained
    
    the training dataset
    
    PEFT configurations
    
    and the method for converting the training data into a "prompt"

###### What do the following parameters mean?
    model: This can be a pre-trained model, a PyTorch module, or a string representing the model name. It specifies the model to be trained.
    
    train_dataset: This is the Dataset used for training.
    
    peft_config: This is the PEFT (Plug and Play Language Model Fine-Tuning) library configuration. It allows users to train adapters and share them on the Hub.
    
    max_seq_length: It defines the maximum sequence length for the ConstantLengthDataset and for automatically creating the Dataset. The default value is 512.
    
    tokenizer: This is the tokenizer used for training. If not specified, the tokenizer associated with the model will be used.
    
    packing indicates whether multiple short examples should be packed into the same input sequence to increase training efficiency. This is done using the ConstantLengthDataset utility 
    class.
    
    formatting_func: This function is used for formatting the text before tokenization. It's commonly used for instruction fine-tuning where datasets might have separate columns for 
    prompts and responses.



        from trl import SFTTrainer
        trainer = SFTTrainer(
            model=model,
            tokenizer=tokenizer,
            args=training_arguments,
            train_dataset=generated_train_dataset,
            eval_dataset=generated_val_dataset,
            peft_config=lora_config,
            dataset_text_field="text",   
        )


Now we are ready to train the model

        trainer.train()


#### Post process after training the model

After complete the training the model i saved the model on the local directory. Now

        model.save_pretrained("mistral")
        tokenizer.save_pretrained("mistral")

Then i need to test how my model perform on the new dataset. First i need to load the saved model from the diretory.

        instruct_model = AutoModelForCausalLM.from_pretrained("checkpoint-50", torch_dtype=torch.bfloat16)
        instruct_tokenizer = AutoTokenizer.from_pretrained(
            "checkpoint-50", 
            padding_side = "right",
            add_eos_token = True,
        )
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.add_bos_token, tokenizer.add_eos_token

For testing the performance of  the model i am going to use the pipenine from transformers library.

        new_pipe = pipeline(
            "text-generation", 
            model=instruct_model, 
            tokenizer = instruct_tokenizer, 
            torch_dtype=torch.bfloat16, 
            device_map="cuda"
        )
        
        next_sequences = new_pipe(
            f"""<s>[INST]{instruction}
            {f"Here is some context: {cntext}"}[/INST]</s>""" ,
            
            do_sample=True,
            max_new_tokens=256, 
            temperature=0.7, 
            top_k=50, 
            top_p=0.95,
            num_return_sequences=1,
        )
        print(next_sequences[0]['generated_text'])

Now we can see our result. 


